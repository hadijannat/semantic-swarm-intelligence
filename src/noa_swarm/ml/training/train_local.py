"""Local training script for CharCNN model.

This script trains the CharCNN model on synthetic tag data generated by
SyntheticTagGenerator. It supports:
- Multi-head training (property_class + signal_role)
- Validation during training
- Model checkpointing
- Logging of metrics

Usage:
    python -m noa_swarm.ml.training.train_local --epochs 50 --batch-size 64

Example:
    >>> from noa_swarm.ml.training.train_local import train_model
    >>> model, history = train_model(num_samples=1000, epochs=10)
"""

from __future__ import annotations

import argparse
import logging
from dataclasses import dataclass, field
from pathlib import Path
from typing import TYPE_CHECKING

import torch
import torch.nn as nn
from torch.utils.data import DataLoader, Dataset

from noa_swarm.ml.datasets import SyntheticTagGenerator, TagSample
from noa_swarm.ml.models.charcnn import (
    CharacterTokenizer,
    CharCNN,
    CharCNNConfig,
    get_property_class_from_category,
    get_signal_role_from_prefix,
)
from noa_swarm.ml.training.eval import compute_accuracy, compute_macro_f1

if TYPE_CHECKING:
    from collections.abc import Sequence

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s - %(levelname)s - %(message)s",
)
logger = logging.getLogger(__name__)


@dataclass
class TrainingConfig:
    """Configuration for training.

    Attributes:
        num_samples: Total number of synthetic samples to generate
        train_ratio: Fraction for training
        val_ratio: Fraction for validation
        batch_size: Training batch size
        epochs: Number of training epochs
        learning_rate: Initial learning rate
        weight_decay: Weight decay for optimizer
        lr_schedule: Learning rate schedule type ("step", "cosine", or None)
        lr_step_size: Step size for StepLR scheduler
        lr_gamma: Gamma for StepLR scheduler
        checkpoint_dir: Directory to save checkpoints
        save_best: Save best model based on validation loss
        seed: Random seed for reproducibility
        device: Device to train on ("cpu", "cuda", "mps")
        log_interval: Log every N batches
    """

    num_samples: int = 10000
    train_ratio: float = 0.8
    val_ratio: float = 0.1
    batch_size: int = 64
    epochs: int = 50
    learning_rate: float = 1e-3
    weight_decay: float = 1e-4
    lr_schedule: str | None = "step"
    lr_step_size: int = 20
    lr_gamma: float = 0.5
    checkpoint_dir: Path = field(default_factory=lambda: Path("checkpoints"))
    save_best: bool = True
    seed: int = 42
    device: str = "cpu"
    log_interval: int = 10


@dataclass
class TrainingHistory:
    """Training history and metrics.

    Attributes:
        train_loss: Training loss per epoch
        val_loss: Validation loss per epoch
        train_property_acc: Training property accuracy per epoch
        val_property_acc: Validation property accuracy per epoch
        train_signal_acc: Training signal accuracy per epoch
        val_signal_acc: Validation signal accuracy per epoch
        best_epoch: Epoch with best validation loss
        best_val_loss: Best validation loss achieved
    """

    train_loss: list[float] = field(default_factory=list)
    val_loss: list[float] = field(default_factory=list)
    train_property_acc: list[float] = field(default_factory=list)
    val_property_acc: list[float] = field(default_factory=list)
    train_signal_acc: list[float] = field(default_factory=list)
    val_signal_acc: list[float] = field(default_factory=list)
    best_epoch: int = 0
    best_val_loss: float = float("inf")


class TagDataset(Dataset):  # type: ignore[type-arg]
    """PyTorch Dataset for tag samples.

    Converts TagSample objects to tensors suitable for CharCNN training.
    """

    def __init__(
        self,
        samples: Sequence[TagSample],
        tokenizer: CharacterTokenizer,
    ) -> None:
        """Initialize the dataset.

        Args:
            samples: List of TagSample objects
            tokenizer: CharacterTokenizer for encoding tag names
        """
        self.samples = list(samples)
        self.tokenizer = tokenizer

        # Pre-compute labels
        self.inputs: list[torch.Tensor] = []
        self.property_labels: list[int] = []
        self.signal_labels: list[int] = []

        for sample in self.samples:
            self.inputs.append(self.tokenizer.encode(sample.tag_name))

            # Get property class from category in features
            category = sample.features.get("category", "unknown")
            self.property_labels.append(get_property_class_from_category(category))

            # Get signal role from prefix in features
            prefix = sample.features.get("prefix", "")
            self.signal_labels.append(get_signal_role_from_prefix(prefix))

    def __len__(self) -> int:
        """Return the number of samples."""
        return len(self.samples)

    def __getitem__(self, idx: int) -> tuple[torch.Tensor, torch.Tensor, torch.Tensor]:
        """Get a single sample.

        Returns:
            Tuple of (input, property_label, signal_label)
        """
        return (
            self.inputs[idx],
            torch.tensor(self.property_labels[idx], dtype=torch.long),
            torch.tensor(self.signal_labels[idx], dtype=torch.long),
        )


def create_dataloaders(
    config: TrainingConfig,
    model_config: CharCNNConfig,
) -> tuple[DataLoader, DataLoader, DataLoader]:  # type: ignore[type-arg]
    """Create train, validation, and test dataloaders.

    Args:
        config: Training configuration
        model_config: Model configuration

    Returns:
        Tuple of (train_loader, val_loader, test_loader)
    """
    # Generate synthetic data
    generator = SyntheticTagGenerator(seed=config.seed)
    split = generator.generate_split(
        total=config.num_samples,
        train_ratio=config.train_ratio,
        val_ratio=config.val_ratio,
    )

    logger.info(
        f"Generated {split.total_size} samples: "
        f"train={len(split.train)}, val={len(split.val)}, test={len(split.test)}"
    )

    # Create tokenizer
    tokenizer = CharacterTokenizer(
        alphabet=model_config.alphabet,
        max_length=model_config.max_seq_length,
    )

    # Create datasets
    train_dataset = TagDataset(split.train, tokenizer)
    val_dataset = TagDataset(split.val, tokenizer)
    test_dataset = TagDataset(split.test, tokenizer)

    # Create dataloaders
    train_loader: DataLoader = DataLoader(  # type: ignore[type-arg]
        train_dataset,
        batch_size=config.batch_size,
        shuffle=True,
        num_workers=0,
        drop_last=True,
    )
    val_loader: DataLoader = DataLoader(  # type: ignore[type-arg]
        val_dataset,
        batch_size=config.batch_size,
        shuffle=False,
        num_workers=0,
    )
    test_loader: DataLoader = DataLoader(  # type: ignore[type-arg]
        test_dataset,
        batch_size=config.batch_size,
        shuffle=False,
        num_workers=0,
    )

    return train_loader, val_loader, test_loader


def train_epoch(
    model: CharCNN,
    train_loader: DataLoader,  # type: ignore[type-arg]
    optimizer: torch.optim.Optimizer,
    criterion: nn.CrossEntropyLoss,
    device: torch.device,
    log_interval: int = 10,
) -> tuple[float, float, float]:
    """Train for one epoch.

    Args:
        model: The CharCNN model
        train_loader: Training data loader
        optimizer: Optimizer
        criterion: Loss function
        device: Device to train on
        log_interval: Log every N batches

    Returns:
        Tuple of (avg_loss, property_accuracy, signal_accuracy)
    """
    model.train()
    total_loss = 0.0
    all_property_preds: list[torch.Tensor] = []
    all_property_targets: list[torch.Tensor] = []
    all_signal_preds: list[torch.Tensor] = []
    all_signal_targets: list[torch.Tensor] = []

    for batch_idx, (inputs, property_targets, signal_targets) in enumerate(train_loader):
        inputs = inputs.to(device)
        property_targets = property_targets.to(device)
        signal_targets = signal_targets.to(device)

        optimizer.zero_grad()

        outputs = model(inputs)

        # Compute losses for both heads
        property_loss = criterion(outputs["property_class"], property_targets)
        signal_loss = criterion(outputs["signal_role"], signal_targets)

        # Combined loss (equal weighting)
        loss = property_loss + signal_loss

        loss.backward()
        optimizer.step()

        total_loss += loss.item()

        # Track predictions
        property_preds = outputs["property_class"].argmax(dim=-1)
        signal_preds = outputs["signal_role"].argmax(dim=-1)

        all_property_preds.append(property_preds.cpu())
        all_property_targets.append(property_targets.cpu())
        all_signal_preds.append(signal_preds.cpu())
        all_signal_targets.append(signal_targets.cpu())

        if (batch_idx + 1) % log_interval == 0:
            logger.debug(
                f"  Batch {batch_idx + 1}/{len(train_loader)}, " f"Loss: {loss.item():.4f}"
            )

    # Compute epoch metrics
    avg_loss = total_loss / len(train_loader)
    property_acc = compute_accuracy(
        torch.cat(all_property_preds),
        torch.cat(all_property_targets),
    )
    signal_acc = compute_accuracy(
        torch.cat(all_signal_preds),
        torch.cat(all_signal_targets),
    )

    return avg_loss, property_acc, signal_acc


def validate(
    model: CharCNN,
    val_loader: DataLoader,  # type: ignore[type-arg]
    criterion: nn.CrossEntropyLoss,
    device: torch.device,
) -> tuple[float, float, float, float, float]:
    """Validate the model.

    Args:
        model: The CharCNN model
        val_loader: Validation data loader
        criterion: Loss function
        device: Device to validate on

    Returns:
        Tuple of (avg_loss, property_accuracy, property_f1, signal_accuracy, signal_f1)
    """
    model.eval()
    total_loss = 0.0
    all_property_preds: list[torch.Tensor] = []
    all_property_targets: list[torch.Tensor] = []
    all_signal_preds: list[torch.Tensor] = []
    all_signal_targets: list[torch.Tensor] = []

    with torch.no_grad():
        for inputs, property_targets, signal_targets in val_loader:
            inputs = inputs.to(device)
            property_targets = property_targets.to(device)
            signal_targets = signal_targets.to(device)

            outputs = model(inputs)

            property_loss = criterion(outputs["property_class"], property_targets)
            signal_loss = criterion(outputs["signal_role"], signal_targets)
            loss = property_loss + signal_loss

            total_loss += loss.item()

            property_preds = outputs["property_class"].argmax(dim=-1)
            signal_preds = outputs["signal_role"].argmax(dim=-1)

            all_property_preds.append(property_preds.cpu())
            all_property_targets.append(property_targets.cpu())
            all_signal_preds.append(signal_preds.cpu())
            all_signal_targets.append(signal_targets.cpu())

    # Compute metrics
    avg_loss = total_loss / len(val_loader)

    property_preds_cat = torch.cat(all_property_preds)
    property_targets_cat = torch.cat(all_property_targets)
    signal_preds_cat = torch.cat(all_signal_preds)
    signal_targets_cat = torch.cat(all_signal_targets)

    property_acc = compute_accuracy(property_preds_cat, property_targets_cat)
    property_f1 = compute_macro_f1(property_preds_cat, property_targets_cat)
    signal_acc = compute_accuracy(signal_preds_cat, signal_targets_cat)
    signal_f1 = compute_macro_f1(signal_preds_cat, signal_targets_cat)

    return avg_loss, property_acc, property_f1, signal_acc, signal_f1


def save_checkpoint(
    model: CharCNN,
    optimizer: torch.optim.Optimizer,
    epoch: int,
    history: TrainingHistory,
    config: TrainingConfig,
    model_config: CharCNNConfig,
    filepath: Path,
) -> None:
    """Save a training checkpoint.

    Args:
        model: The CharCNN model
        optimizer: Optimizer
        epoch: Current epoch
        history: Training history
        config: Training configuration
        model_config: Model configuration
        filepath: Path to save checkpoint
    """
    filepath.parent.mkdir(parents=True, exist_ok=True)

    checkpoint = {
        "epoch": epoch,
        "model_state_dict": model.state_dict(),
        "optimizer_state_dict": optimizer.state_dict(),
        "history": history,
        "config": config,
        "model_config": model_config,
    }

    torch.save(checkpoint, filepath)
    logger.info(f"Saved checkpoint to {filepath}")


def load_checkpoint(
    filepath: Path,
    device: torch.device | str = "cpu",
) -> tuple[CharCNN, torch.optim.Optimizer, int, TrainingHistory]:
    """Load a training checkpoint.

    Args:
        filepath: Path to checkpoint file
        device: Device to load model on

    Returns:
        Tuple of (model, optimizer, epoch, history)
    """
    checkpoint = torch.load(filepath, map_location=device, weights_only=False)

    model_config = checkpoint["model_config"]
    model = CharCNN(model_config)
    model.load_state_dict(checkpoint["model_state_dict"])
    model.to(device)

    optimizer = torch.optim.Adam(model.parameters())
    optimizer.load_state_dict(checkpoint["optimizer_state_dict"])

    return model, optimizer, checkpoint["epoch"], checkpoint["history"]


def train_model(
    config: TrainingConfig | None = None,
    model_config: CharCNNConfig | None = None,
) -> tuple[CharCNN, TrainingHistory]:
    """Train the CharCNN model.

    Args:
        config: Training configuration (uses defaults if None)
        model_config: Model configuration (uses defaults if None)

    Returns:
        Tuple of (trained_model, training_history)
    """
    config = config or TrainingConfig()
    model_config = model_config or CharCNNConfig()

    # Set random seeds
    torch.manual_seed(config.seed)

    # Setup device
    if config.device == "cuda" and torch.cuda.is_available():
        device = torch.device("cuda")
    elif config.device == "mps" and torch.backends.mps.is_available():
        device = torch.device("mps")
    else:
        device = torch.device("cpu")

    logger.info(f"Training on device: {device}")

    # Create dataloaders
    train_loader, val_loader, test_loader = create_dataloaders(config, model_config)

    # Create model
    model = CharCNN(model_config)
    model.to(device)
    logger.info(f"Model parameters: {sum(p.numel() for p in model.parameters()):,}")

    # Create optimizer and criterion
    optimizer = torch.optim.Adam(
        model.parameters(),
        lr=config.learning_rate,
        weight_decay=config.weight_decay,
    )

    criterion = nn.CrossEntropyLoss()

    # Learning rate scheduler
    scheduler: torch.optim.lr_scheduler.LRScheduler | None = None
    if config.lr_schedule == "step":
        scheduler = torch.optim.lr_scheduler.StepLR(
            optimizer,
            step_size=config.lr_step_size,
            gamma=config.lr_gamma,
        )
    elif config.lr_schedule == "cosine":
        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(
            optimizer,
            T_max=config.epochs,
        )

    # Training history
    history = TrainingHistory()

    # Training loop
    for epoch in range(1, config.epochs + 1):
        logger.info(f"Epoch {epoch}/{config.epochs}")

        # Train
        train_loss, train_prop_acc, train_sig_acc = train_epoch(
            model, train_loader, optimizer, criterion, device, config.log_interval
        )

        # Validate
        val_loss, val_prop_acc, val_prop_f1, val_sig_acc, val_sig_f1 = validate(
            model, val_loader, criterion, device
        )

        # Update learning rate
        if scheduler is not None:
            scheduler.step()

        # Record history
        history.train_loss.append(train_loss)
        history.val_loss.append(val_loss)
        history.train_property_acc.append(train_prop_acc)
        history.val_property_acc.append(val_prop_acc)
        history.train_signal_acc.append(train_sig_acc)
        history.val_signal_acc.append(val_sig_acc)

        # Log metrics
        logger.info(
            f"  Train Loss: {train_loss:.4f}, "
            f"Property Acc: {train_prop_acc:.4f}, "
            f"Signal Acc: {train_sig_acc:.4f}"
        )
        logger.info(
            f"  Val Loss: {val_loss:.4f}, "
            f"Property Acc: {val_prop_acc:.4f} (F1: {val_prop_f1:.4f}), "
            f"Signal Acc: {val_sig_acc:.4f} (F1: {val_sig_f1:.4f})"
        )

        # Save best model
        if config.save_best and val_loss < history.best_val_loss:
            history.best_val_loss = val_loss
            history.best_epoch = epoch
            save_checkpoint(
                model,
                optimizer,
                epoch,
                history,
                config,
                model_config,
                config.checkpoint_dir / "best_model.pt",
            )

    # Final evaluation on test set
    logger.info("Final evaluation on test set:")
    test_loss, test_prop_acc, test_prop_f1, test_sig_acc, test_sig_f1 = validate(
        model, test_loader, criterion, device
    )
    logger.info(
        f"  Test Loss: {test_loss:.4f}, "
        f"Property Acc: {test_prop_acc:.4f} (F1: {test_prop_f1:.4f}), "
        f"Signal Acc: {test_sig_acc:.4f} (F1: {test_sig_f1:.4f})"
    )

    logger.info(
        f"Best model at epoch {history.best_epoch} " f"with val loss {history.best_val_loss:.4f}"
    )

    return model, history


def main() -> None:
    """Main entry point for training script."""
    parser = argparse.ArgumentParser(description="Train CharCNN model on synthetic tags")

    # Training arguments
    parser.add_argument(
        "--num-samples",
        type=int,
        default=10000,
        help="Number of synthetic samples to generate",
    )
    parser.add_argument(
        "--epochs",
        type=int,
        default=50,
        help="Number of training epochs",
    )
    parser.add_argument(
        "--batch-size",
        type=int,
        default=64,
        help="Batch size",
    )
    parser.add_argument(
        "--lr",
        type=float,
        default=1e-3,
        help="Learning rate",
    )
    parser.add_argument(
        "--device",
        type=str,
        default="cpu",
        choices=["cpu", "cuda", "mps"],
        help="Device to train on",
    )
    parser.add_argument(
        "--checkpoint-dir",
        type=str,
        default="checkpoints",
        help="Directory to save checkpoints",
    )
    parser.add_argument(
        "--seed",
        type=int,
        default=42,
        help="Random seed",
    )

    # Model arguments
    parser.add_argument(
        "--embedding-dim",
        type=int,
        default=64,
        help="Character embedding dimension",
    )
    parser.add_argument(
        "--hidden-dim",
        type=int,
        default=256,
        help="Hidden layer dimension",
    )
    parser.add_argument(
        "--max-seq-length",
        type=int,
        default=32,
        help="Maximum sequence length",
    )

    args = parser.parse_args()

    # Create configurations
    config = TrainingConfig(
        num_samples=args.num_samples,
        epochs=args.epochs,
        batch_size=args.batch_size,
        learning_rate=args.lr,
        device=args.device,
        checkpoint_dir=Path(args.checkpoint_dir),
        seed=args.seed,
    )

    model_config = CharCNNConfig(
        embedding_dim=args.embedding_dim,
        hidden_dim=args.hidden_dim,
        max_seq_length=args.max_seq_length,
    )

    # Train
    train_model(config, model_config)


if __name__ == "__main__":
    main()
